{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score, classification_report\n",
    "import nltk\n",
    "import tkinter \n",
    "from tkinter.filedialog import askopenfilename\n",
    "import joblib\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import pickle\n",
    "#on va utiliser sur le main  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    #remove punctuations\n",
    "    punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ''' + string.punctuation\n",
    "\n",
    "    # Arabic stop words with nltk\n",
    "    stop_words = stopwords.words()\n",
    "\n",
    "    arabic_diacritics = re.compile(\"\"\"\n",
    "                                 ّ    | # Shadda\n",
    "                                 َ    | # Fatha\n",
    "                                 ً    | # Tanwin Fath\n",
    "                                 ُ    | # Damma\n",
    "                                 ٌ    | # Tanwin Damm\n",
    "                                 ِ    | # Kasra\n",
    "                                 ٍ    | # Tanwin Kasr\n",
    "                                 ْ    | # Sukun\n",
    "                                 ـ     # Tatwil/Kashida\n",
    "                             \"\"\", re.VERBOSE)\n",
    "\n",
    "    translator = str.maketrans('', '', punctuations)\n",
    "    text = text.translate(translator)\n",
    "    \n",
    "    # remove Tashkeel\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    #remove repeating char\n",
    "    text=re.sub(r'(.)\\1+', r'\\1', text) ##hadi zdtha l9itha\n",
    "    \n",
    "    #remove longation\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    text=re.sub(\"شش\",\"ش\",text) ##hadu li lt7t zdthum\n",
    "    text=re.sub(\"يع يع يع\",\"يع\",text) #\n",
    "    text=re.sub(\"ووو\",\"و\",text) #\n",
    "    text=re.sub(\"ييي\",\"ي\",text) #\n",
    "    text=re.sub(\"رر\",\"ر\",text) #\n",
    "    text=re.sub(\"تت\",\"ت\",text) #\n",
    "\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "\n",
    "    return text\n",
    "def countvectorizer():\n",
    "    \n",
    "    data.loc[data['Sentiment']=='Positive',\"Sentiment\"]=1  ##hna bdelt blasst positive ghatwli 1\n",
    "    data.loc[data['Sentiment']=='Negative',\"Sentiment\"]=0##w hna 0\n",
    "    ##splitting\n",
    "    data_X=data[\"Feed\"]\n",
    "    data_Y=data[\"Sentiment\"]\n",
    "    ##convertion du text en nombre sans perdre d'informations\n",
    "    CV=CountVectorizer()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_X,data_Y)\n",
    "    x_traincv=CV.fit_transform(X_train)\n",
    "    return x_traincv\n",
    "    \n",
    "def logistic_regression():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    \n",
    "    # splitting the data into target and feature\n",
    "    feature = data.Feed\n",
    "    target = data.Sentiment\n",
    "    # splitting into train and tests\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
    "\n",
    "    # make pipeline\n",
    "    pipe = make_pipeline(TfidfVectorizer(),\n",
    "                        LogisticRegression())\n",
    "    # make param grid\n",
    "    param_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "    # create and fit the model\n",
    "    model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    model.fit(X_train,Y_train)\n",
    "    # make prediction and print accuracy\n",
    "    prediction = model.predict(X_test)\n",
    "    print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
    "    print(classification_report(Y_test, prediction))\n",
    "    #pkl\n",
    "    joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "    NB_spam_model = open('logistic_regression_model.pkl','rb')\n",
    "    clf = joblib.load(NB_spam_model)\n",
    "def classifier_bayes():\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    data['label'] = data['Sentiment'].map({'Positive': 0, 'Negative': 1})\n",
    "    feature=data[\"Feed\"]\n",
    "    target=data[\"label\"]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
    "\n",
    "    b_model= make_pipeline(TfidfVectorizer(),\n",
    "                        MultinomialNB())\n",
    "    b_model.fit(X_train,Y_train)\n",
    "    prediction = b_model.predict(X_test)\n",
    "    #pkl\n",
    "    print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
    "    print(classification_report(Y_test, prediction))\n",
    "    #joblib.dump(b_model, 'NB_model.pkl')\n",
    "    #NB_spam_model = open('NB_model.pkl','rb')\n",
    "    #clf = joblib.load(NB_spam_model)                   \n",
    "                      \n",
    "\n",
    "def random_forest():\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    feature = data.Feed\n",
    "    target = data.Sentiment\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
    "\n",
    "    pipe = make_pipeline(TfidfVectorizer(),\n",
    "                        RandomForestClassifier())\n",
    "    \n",
    "    param_grid = {'randomforestclassifier__n_estimators':[10, 100, 1000],\n",
    "                 'randomforestclassifier__max_features':['sqrt', 'log2']}\n",
    "    \n",
    "    rf_model = GridSearchCV(pipe, param_grid, cv=5)\n",
    "    rf_model.fit(X_train,Y_train)\n",
    "    \n",
    "    prediction = rf_model.predict(X_test)\n",
    "    print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
    "    print(classification_report(Y_test, prediction))\n",
    "\n",
    "    #pkl\n",
    "    joblib.dump(rf_model, 'random_forest.pkl')\n",
    "    NB_spam_model = open('random_forest.pkl','rb')\n",
    "    clf = joblib.load(NB_spam_model)\n",
    "    \n",
    "    \n",
    "\n",
    "def super_vector_mach():\n",
    "    from sklearn.svm import SVC\n",
    "    feature = data.Feed\n",
    "    target = data.Sentiment\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(feature, target, test_size =.2, random_state=100)\n",
    "    pipe = make_pipeline(TfidfVectorizer(),SVC())\n",
    "    param_grid = {'svc__kernel': ['rbf', 'linear', 'poly'],\n",
    "                 'svc__gamma': [0.1, 1, 10, 100],\n",
    "                 'svc__C': [0.1, 1, 10, 100]}\n",
    "\n",
    "    svc_model = GridSearchCV(pipe, param_grid, cv=3)\n",
    "    svc_model.fit(X_train, Y_train)\n",
    "\n",
    "    prediction = svc_model.predict(X_test)\n",
    "    #pkl\n",
    "    joblib.dump(svc_model, 'super_vector_mach.pkl')\n",
    "    NB_spam_model = open('super_vector_mach.pkl','rb')\n",
    "    clf = joblib.load(NB_spam_model)\n",
    "    print(f\"Accuracy score is {accuracy_score(Y_test, prediction):.2f}\")\n",
    "    print(classification_report(Y_test, prediction))\n",
    "#l'utilisation de l'interface    \n",
    "def Export_fichier_interface(repertoireinit):\n",
    "    root=tkinter.Tk()\n",
    "    filename=tkinter.filedialog.askopenfilename(initialdir=repertoireinit,title=\"choisir votre fichier\",filetypes=((\"Tous type de fichier\",\"*.*\"),))\n",
    "    root.destroy()\n",
    "    return filename  \n",
    "    \n",
    "pass \n",
    "repertoireinit='c:\\\\'\n",
    "nom_fich=Export_fichier_interface(repertoireinit)\n",
    "data = pd.read_excel(nom_fich)\n",
    "#definir la ponctuation:\n",
    "\n",
    "data['Feed'] = data['Feed'].apply(preprocess) # appliquerau contenu tashkeel...\n",
    "#vous pouvez appelez chaque fonctions vous voulez par exemple:\n",
    "print('1 : pour classification logistic regression')\n",
    "print('2 : pour classification Bayes')\n",
    "print('3 : pour classification Support Vector Machines ')\n",
    "print('4 : pour classification Random Forest')\n",
    "a=int(input(print('tapez une valeur :')))\n",
    "\n",
    "if a==1:\n",
    "    logistic_regression()\n",
    "if a==2:\n",
    "    classifier_bayes()\n",
    "if a==3: \n",
    "    super_vector_mach()\n",
    "if a==4:\n",
    "    random_forest()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
